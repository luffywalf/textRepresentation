##### 网上blog阅读 自己整理的笔记 参考见下
参考：https://www.jianshu.com/p/40fbe2bdffd3

### Topic Model
* 就是一系列相关的概念 就像一个桶 和这个主题相关度很高的词会被装进这个桶里； 
* 用数学角度说，主题就是词汇表上词语的条件概率分布，与主题密切相关的词，条件概率p(w|z)越大
* 一个词可以装到不同的桶里，一段文字往往含有多个主题
* p(w|d)=∑p(w|z)p(z|d) 
* 用途：
    * 计算文本相似性，考虑到文本语义，避免多义词，同义词的影响
    * 文本聚类，用户聚类
    * 去除噪音，保留最正确的主题，更好的刻画文本主题
          
##### TFIDF
如何自己写以及如何用tfidfvectorizor都比较熟了

##### 基于矩阵分解的LFM - latent factor model
参考：https://yq.aliyun.com/articles/152083
* 常用于推荐系统
* 原理就是用中间的主题latent factor将user-item矩阵分解，即将user, item通过topic联系起来，变为P矩阵：user * topic 和Q矩阵：topic * item，通过梯度下降法不断用P * Q去逼近原本的user-item矩阵 数学上大概是 矩阵的uv分解，将很大的一个稀疏矩阵用两个低维的矩阵去估计逼近～

##### LSI/LSA   (Latent Semantic Indexing/Latent Semantic Analysis)
依据整理：https://www.cnblogs.com/pinard/p/6805861.html

* SVD矩阵奇异值分解
    * http://www.cnblogs.com/pinard/p/6251584.html
    * 大概就是将非方阵的 m * n矩阵分解成 A * B * C 三个矩阵，其中B是奇异值矩阵，B中往往前k个最大的奇异值就可以占所有奇异值和的大部分，所以常去前k个最大的奇异值对应的矩阵，及其对应的左右奇异矩阵来近似描述原本的矩阵，可以用于降维很方便。

* 就是利用了SVD矩阵分解
* 优：它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再用
* 劣：
    * 1）SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。
    * 2）主题值的选取对结果的影响非常大，很难选择合适的k值。
    * 3）LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。
    * 对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP
    
    
##### pLSI
需要用到EM算法推导 暂时没看
https://blog.csdn.net/yangliuy/article/details/8330640
https://blog.csdn.net/pipisorry/article/details/42560693

然后在plsi上加上贝叶斯框架就是lda了？
    
   
    
















